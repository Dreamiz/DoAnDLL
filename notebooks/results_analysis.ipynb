{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74cdec4a",
   "metadata": {},
   "source": [
    "# 📈 Results Analysis & Visualization\n",
    "\n",
    "This notebook provides comprehensive analysis and visualization of training results, model performance, and evaluation metrics.\n",
    "\n",
    "## 🎯 Objectives\n",
    "- Analyze training curves and convergence\n",
    "- Evaluate model performance across different metrics\n",
    "- Visualize attention mechanisms and feature representations\n",
    "- Generate publication-ready figures and reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from utils.metrics import MetricsCalculator\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📦 All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce8b60",
   "metadata": {},
   "source": [
    "## 📂 Load Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training results (simulated for demonstration)\n",
    "def generate_training_curves(num_epochs=100):\n",
    "    \"\"\"Generate realistic training curves for demonstration\"\"\"\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "    \n",
    "    # Training loss with exponential decay + noise\n",
    "    train_loss = 2.5 * np.exp(-epochs/20) + 0.1 + 0.05 * np.random.randn(num_epochs)\n",
    "    train_loss = np.maximum(train_loss, 0.05)  # Minimum loss\n",
    "    \n",
    "    # Validation loss with some overfitting pattern\n",
    "    val_loss = 2.3 * np.exp(-epochs/25) + 0.15 + 0.03 * np.random.randn(num_epochs)\n",
    "    # Add slight overfitting after epoch 60\n",
    "    overfitting_start = 60\n",
    "    val_loss[overfitting_start:] += 0.001 * (epochs[overfitting_start:] - overfitting_start)\n",
    "    val_loss = np.maximum(val_loss, 0.08)\n",
    "    \n",
    "    # Training accuracy with sigmoid growth\n",
    "    train_acc = 0.95 / (1 + np.exp(-(epochs-20)/10)) + 0.02 * np.random.randn(num_epochs)\n",
    "    train_acc = np.clip(train_acc, 0.1, 0.98)\n",
    "    \n",
    "    # Validation accuracy\n",
    "    val_acc = 0.92 / (1 + np.exp(-(epochs-25)/12)) + 0.015 * np.random.randn(num_epochs)\n",
    "    val_acc = np.clip(val_acc, 0.1, 0.95)\n",
    "    \n",
    "    # Learning rate schedule (cosine annealing)\n",
    "    initial_lr = 1e-4\n",
    "    min_lr = 1e-6\n",
    "    learning_rate = min_lr + (initial_lr - min_lr) * (1 + np.cos(np.pi * epochs / num_epochs)) / 2\n",
    "    \n",
    "    return {\n",
    "        'epoch': epochs,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'train_accuracy': train_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "\n",
    "# Generate training curves\n",
    "training_results = generate_training_curves(100)\n",
    "results_df = pd.DataFrame(training_results)\n",
    "\n",
    "print(\"📊 Training results loaded:\")\n",
    "print(f\"- Number of epochs: {len(results_df)}\")\n",
    "print(f\"- Final train accuracy: {results_df['train_accuracy'].iloc[-1]:.4f}\")\n",
    "print(f\"- Final val accuracy: {results_df['val_accuracy'].iloc[-1]:.4f}\")\n",
    "print(f\"- Best val accuracy: {results_df['val_accuracy'].max():.4f}\")\n",
    "print(f\"- Final train loss: {results_df['train_loss'].iloc[-1]:.4f}\")\n",
    "print(f\"- Final val loss: {results_df['val_loss'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee6b95",
   "metadata": {},
   "source": [
    "## 📈 Training Curves Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training curves plot\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\"Loss Curves\", \"Accuracy Curves\", \"Learning Rate Schedule\", \"Overfitting Analysis\"],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Loss curves\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['epoch'], y=results_df['train_loss'], \n",
    "               mode='lines', name='Train Loss', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['epoch'], y=results_df['val_loss'], \n",
    "               mode='lines', name='Val Loss', line=dict(color='red')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Accuracy curves\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['epoch'], y=results_df['train_accuracy'], \n",
    "               mode='lines', name='Train Accuracy', line=dict(color='green')),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['epoch'], y=results_df['val_accuracy'], \n",
    "               mode='lines', name='Val Accuracy', line=dict(color='orange')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Learning rate schedule\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['epoch'], y=results_df['learning_rate'], \n",
    "               mode='lines', name='Learning Rate', line=dict(color='purple')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Overfitting analysis (gap between train and val accuracy)\n",
    "accuracy_gap = results_df['train_accuracy'] - results_df['val_accuracy']\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['epoch'], y=accuracy_gap, \n",
    "               mode='lines', name='Accuracy Gap', line=dict(color='red')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Add best validation accuracy marker\n",
    "best_val_epoch = results_df['val_accuracy'].idxmax() + 1\n",
    "best_val_acc = results_df['val_accuracy'].max()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[best_val_epoch], y=[best_val_acc], \n",
    "               mode='markers', name=f'Best Val Acc (Epoch {best_val_epoch})',\n",
    "               marker=dict(color='red', size=12, symbol='star')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800, \n",
    "    title_text=\"📈 Training Progress Analysis\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Learning Rate\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy Gap\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Print key statistics\n",
    "print(\"\\n📊 Key Training Statistics:\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_val_epoch}\")\n",
    "print(f\"Final accuracy gap: {accuracy_gap.iloc[-1]:.4f}\")\n",
    "print(f\"Average accuracy gap: {accuracy_gap.mean():.4f}\")\n",
    "print(f\"Loss reduction (train): {results_df['train_loss'].iloc[0]:.4f} → {results_df['train_loss'].iloc[-1]:.4f}\")\n",
    "print(f\"Loss reduction (val): {results_df['val_loss'].iloc[0]:.4f} → {results_df['val_loss'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47812c4",
   "metadata": {},
   "source": [
    "## 🎯 Performance Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic evaluation results for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate test set evaluation\n",
    "num_classes = 50  # Reduced for visualization\n",
    "num_samples = 1000\n",
    "\n",
    "# Generate true labels (with some class imbalance)\n",
    "class_weights = np.random.exponential(2, num_classes)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "y_true = np.random.choice(num_classes, size=num_samples, p=class_weights)\n",
    "\n",
    "# Generate predictions (with realistic accuracy pattern)\n",
    "accuracy = 0.92\n",
    "y_pred = y_true.copy()\n",
    "\n",
    "# Introduce some errors\n",
    "num_errors = int((1 - accuracy) * num_samples)\n",
    "error_indices = np.random.choice(num_samples, size=num_errors, replace=False)\n",
    "for idx in error_indices:\n",
    "    # Confused predictions are more likely to be similar classes\n",
    "    true_class = y_true[idx]\n",
    "    nearby_classes = [c for c in range(max(0, true_class-2), min(num_classes, true_class+3)) if c != true_class]\n",
    "    if nearby_classes:\n",
    "        y_pred[idx] = np.random.choice(nearby_classes)\n",
    "    else:\n",
    "        y_pred[idx] = np.random.choice(num_classes)\n",
    "\n",
    "# Generate probability predictions\n",
    "y_pred_proba = np.random.dirichlet(np.ones(num_classes) * 0.1, size=num_samples)\n",
    "# Make correct predictions have higher probability\n",
    "for i, true_class in enumerate(y_true):\n",
    "    y_pred_proba[i, true_class] = max(y_pred_proba[i, true_class], np.random.uniform(0.6, 0.95))\n",
    "    # Renormalize\n",
    "    y_pred_proba[i] = y_pred_proba[i] / y_pred_proba[i].sum()\n",
    "\n",
    "print(f\"📊 Evaluation Dataset:\")\n",
    "print(f\"- Number of samples: {num_samples}\")\n",
    "print(f\"- Number of classes: {num_classes}\")\n",
    "print(f\"- Actual accuracy: {(y_pred == y_true).mean():.4f}\")\n",
    "print(f\"- Class distribution entropy: {-(class_weights * np.log(class_weights + 1e-8)).sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31223c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "metrics_calc = MetricsCalculator(num_classes)\n",
    "\n",
    "# Basic metrics\n",
    "basic_metrics = metrics_calc.calculate_metrics(y_true.tolist(), y_pred.tolist())\n",
    "\n",
    "# Top-k accuracy\n",
    "top1_acc = metrics_calc.calculate_top_k_accuracy(y_true.tolist(), y_pred_proba, k=1)\n",
    "top3_acc = metrics_calc.calculate_top_k_accuracy(y_true.tolist(), y_pred_proba, k=3)\n",
    "top5_acc = metrics_calc.calculate_top_k_accuracy(y_true.tolist(), y_pred_proba, k=5)\n",
    "\n",
    "# Advanced metrics\n",
    "advanced_metrics = metrics_calc.calculate_advanced_metrics(y_true.tolist(), y_pred_proba)\n",
    "\n",
    "# Class balance metrics\n",
    "balance_metrics = metrics_calc.calculate_class_balance_metrics(y_true.tolist())\n",
    "\n",
    "# Per-class metrics\n",
    "per_class_metrics = metrics_calc.calculate_per_class_metrics(y_true.tolist(), y_pred.tolist())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = metrics_calc.get_confusion_matrix(y_true.tolist(), y_pred.tolist())\n",
    "\n",
    "print(\"📊 Comprehensive Metrics:\")\n",
    "print(\"\\n🎯 Basic Metrics:\")\n",
    "for metric, value in basic_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n🏆 Top-K Accuracy:\")\n",
    "print(f\"  Top-1: {top1_acc:.4f}\")\n",
    "print(f\"  Top-3: {top3_acc:.4f}\")\n",
    "print(f\"  Top-5: {top5_acc:.4f}\")\n",
    "\n",
    "print(\"\\n🔍 Advanced Metrics:\")\n",
    "for metric, value in advanced_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n⚖️ Class Balance:\")\n",
    "print(f\"  Imbalance ratio: {balance_metrics['imbalance_ratio']:.2f}\")\n",
    "print(f\"  Gini coefficient: {balance_metrics['gini_coefficient']:.3f}\")\n",
    "print(f\"  Classes present: {balance_metrics['num_classes_present']}/{num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance metrics dashboard\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=3,\n",
    "    subplot_titles=[\"Accuracy Metrics\", \"Precision/Recall/F1\", \"Top-K Accuracy\",\n",
    "                   \"Confidence Distribution\", \"Class Distribution\", \"Confusion Matrix (Sample)\"],\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"histogram\"}, {\"type\": \"bar\"}, {\"type\": \"heatmap\"}]]\n",
    ")\n",
    "\n",
    "# Accuracy metrics\n",
    "acc_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "acc_values = [basic_metrics[m] for m in acc_metrics]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=acc_metrics, y=acc_values, name=\"Accuracy Metrics\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Precision/Recall/F1 comparison\n",
    "prf_metrics = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "prf_values = [basic_metrics[m] for m in prf_metrics]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=prf_metrics, y=prf_values, name=\"PRF Metrics\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Top-K accuracy\n",
    "topk_values = [top1_acc, top3_acc, top5_acc]\n",
    "topk_labels = ['Top-1', 'Top-3', 'Top-5']\n",
    "fig.add_trace(\n",
    "    go.Bar(x=topk_labels, y=topk_values, name=\"Top-K Accuracy\"),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# Confidence distribution\n",
    "max_probs = np.max(y_pred_proba, axis=1)\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=max_probs, nbinsx=30, name=\"Confidence\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Class distribution (top 10 classes)\n",
    "class_counts = np.bincount(y_true)\n",
    "top_classes = np.argsort(class_counts)[-10:]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=[f\"Class {i}\" for i in top_classes], \n",
    "           y=class_counts[top_classes], name=\"Class Counts\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Confusion matrix (subset for visualization)\n",
    "cm_subset = cm[:10, :10]  # Top 10x10 classes\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=cm_subset, colorscale='Blues', name=\"Confusion Matrix\"),\n",
    "    row=2, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"🎯 Performance Metrics Dashboard\")\n",
    "fig.show()\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\n🔍 Additional Analysis:\")\n",
    "print(f\"Mean confidence: {advanced_metrics['mean_confidence']:.3f} ± {advanced_metrics['std_confidence']:.3f}\")\n",
    "print(f\"Prediction entropy: {advanced_metrics['mean_entropy']:.3f} ± {advanced_metrics['std_entropy']:.3f}\")\n",
    "print(f\"Cross-entropy loss: {advanced_metrics['cross_entropy']:.4f}\")\n",
    "\n",
    "# Find most confused classes\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "np.fill_diagonal(cm_norm, 0)  # Remove diagonal\n",
    "most_confused = np.unravel_index(np.argmax(cm_norm), cm_norm.shape)\n",
    "print(f\"Most confused classes: {most_confused[0]} → {most_confused[1]} ({cm_norm[most_confused]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45838606",
   "metadata": {},
   "source": [
    "## 🔍 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed error analysis\n",
    "top_k_errors = metrics_calc.get_top_k_errors(y_true.tolist(), y_pred_proba, k=5, worst_n=10)\n",
    "\n",
    "print(\"🔍 Detailed Error Analysis:\")\n",
    "print(f\"Total incorrect predictions: {top_k_errors['total_incorrect']}\")\n",
    "print(f\"Top-5 recovery rate: {top_k_errors['top_k_recovery_rate']:.3f}\")\n",
    "\n",
    "# Analyze error patterns\n",
    "error_mask = y_pred != y_true\n",
    "error_indices = np.where(error_mask)[0]\n",
    "correct_indices = np.where(~error_mask)[0]\n",
    "\n",
    "# Confidence comparison between correct and incorrect predictions\n",
    "error_confidences = np.max(y_pred_proba[error_indices], axis=1)\n",
    "correct_confidences = np.max(y_pred_proba[correct_indices], axis=1)\n",
    "\n",
    "print(f\"\\n📊 Confidence Analysis:\")\n",
    "print(f\"Mean confidence (correct): {correct_confidences.mean():.3f}\")\n",
    "print(f\"Mean confidence (incorrect): {error_confidences.mean():.3f}\")\n",
    "print(f\"Confidence difference: {correct_confidences.mean() - error_confidences.mean():.3f}\")\n",
    "\n",
    "# Create error analysis plots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\"Confidence: Correct vs Incorrect\", \"Error Distribution by True Class\",\n",
    "                   \"Error Distribution by Predicted Class\", \"Most Common Error Patterns\"]\n",
    ")\n",
    "\n",
    "# Confidence distribution comparison\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=correct_confidences, name=\"Correct\", opacity=0.7, nbinsx=30),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=error_confidences, name=\"Incorrect\", opacity=0.7, nbinsx=30),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Error rate by true class\n",
    "error_by_true_class = []\n",
    "for class_id in range(min(20, num_classes)):  # Limit to 20 classes for visualization\n",
    "    class_mask = y_true == class_id\n",
    "    if class_mask.sum() > 0:\n",
    "        error_rate = (y_pred[class_mask] != y_true[class_mask]).mean()\n",
    "        error_by_true_class.append(error_rate)\n",
    "    else:\n",
    "        error_by_true_class.append(0)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(len(error_by_true_class))), y=error_by_true_class, \n",
    "           name=\"Error Rate by True Class\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Error rate by predicted class\n",
    "error_by_pred_class = []\n",
    "for class_id in range(min(20, num_classes)):\n",
    "    class_mask = y_pred == class_id\n",
    "    if class_mask.sum() > 0:\n",
    "        error_rate = (y_pred[class_mask] != y_true[class_mask]).mean()\n",
    "        error_by_pred_class.append(error_rate)\n",
    "    else:\n",
    "        error_by_pred_class.append(0)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(len(error_by_pred_class))), y=error_by_pred_class,\n",
    "           name=\"Error Rate by Predicted Class\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Most common error patterns\n",
    "error_patterns = {}\n",
    "for i in error_indices:\n",
    "    pattern = (y_true[i], y_pred[i])\n",
    "    error_patterns[pattern] = error_patterns.get(pattern, 0) + 1\n",
    "\n",
    "# Get top 10 error patterns\n",
    "top_error_patterns = sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "pattern_labels = [f\"{true_class}→{pred_class}\" for (true_class, pred_class), count in top_error_patterns]\n",
    "pattern_counts = [count for (true_class, pred_class), count in top_error_patterns]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=pattern_labels, y=pattern_counts, name=\"Error Patterns\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"🔍 Error Analysis Dashboard\")\n",
    "fig.update_xaxes(tickangle=45, row=2, col=2)\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\n🚨 Top 5 Error Patterns:\")\n",
    "for i, ((true_class, pred_class), count) in enumerate(top_error_patterns[:5], 1):\n",
    "    print(f\"  {i}. Class {true_class} → Class {pred_class}: {count} errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3905e9a",
   "metadata": {},
   "source": [
    "## 🧠 Feature Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic feature representations for visualization\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate high-dimensional features (768-dim like BERT/ViT)\n",
    "feature_dim = 768\n",
    "num_samples_viz = 500  # Subset for visualization\n",
    "\n",
    "# Generate class centers in high-dimensional space\n",
    "num_viz_classes = 10\n",
    "class_centers = np.random.randn(num_viz_classes, feature_dim) * 2\n",
    "\n",
    "# Generate features around class centers with some noise\n",
    "features = []\n",
    "labels_viz = []\n",
    "for i in range(num_samples_viz):\n",
    "    class_id = np.random.randint(0, num_viz_classes)\n",
    "    noise = np.random.randn(feature_dim) * 0.5\n",
    "    feature = class_centers[class_id] + noise\n",
    "    features.append(feature)\n",
    "    labels_viz.append(class_id)\n",
    "\n",
    "features = np.array(features)\n",
    "labels_viz = np.array(labels_viz)\n",
    "\n",
    "print(f\"🧠 Feature Analysis:\")\n",
    "print(f\"- Feature dimension: {feature_dim}\")\n",
    "print(f\"- Number of samples: {num_samples_viz}\")\n",
    "print(f\"- Number of classes: {num_viz_classes}\")\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "print(\"\\n🔄 Performing dimensionality reduction...\")\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features)\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "features_tsne = tsne.fit_transform(features[:300])  # Limit for t-SNE speed\n",
    "labels_tsne = labels_viz[:300]\n",
    "\n",
    "print(\"✅ Dimensionality reduction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature spaces\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\"PCA Visualization\", \"t-SNE Visualization\", \n",
    "                   \"Feature Variance Analysis\", \"Class Separability\"]\n",
    ")\n",
    "\n",
    "# PCA visualization\n",
    "colors = px.colors.qualitative.Set3[:num_viz_classes]\n",
    "for class_id in range(num_viz_classes):\n",
    "    mask = labels_viz == class_id\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=features_pca[mask, 0], y=features_pca[mask, 1],\n",
    "                  mode='markers', name=f'Class {class_id}',\n",
    "                  marker=dict(color=colors[class_id], size=6, opacity=0.7)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# t-SNE visualization\n",
    "for class_id in range(num_viz_classes):\n",
    "    mask = labels_tsne == class_id\n",
    "    if mask.sum() > 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=features_tsne[mask, 0], y=features_tsne[mask, 1],\n",
    "                      mode='markers', name=f'Class {class_id}',\n",
    "                      marker=dict(color=colors[class_id], size=6, opacity=0.7),\n",
    "                      showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "# Feature variance analysis\n",
    "feature_vars = np.var(features, axis=0)\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=feature_vars, nbinsx=50, name=\"Feature Variance\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Class separability (silhouette-like analysis)\n",
    "from sklearn.metrics import silhouette_samples\n",
    "silhouette_scores = silhouette_samples(features_pca, labels_viz)\n",
    "silhouette_by_class = [silhouette_scores[labels_viz == i].mean() for i in range(num_viz_classes)]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(range(num_viz_classes)), y=silhouette_by_class,\n",
    "           name=\"Silhouette Score\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"🧠 Feature Space Analysis\")\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\n📊 Feature Analysis Results:\")\n",
    "print(f\"Average silhouette score: {np.mean(silhouette_scores):.3f}\")\n",
    "print(f\"Feature variance range: {feature_vars.min():.3f} - {feature_vars.max():.3f}\")\n",
    "print(f\"PCA first component explains: {pca.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"PCA second component explains: {pca.explained_variance_ratio_[1]:.1%} of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76df1ce",
   "metadata": {},
   "source": [
    "## 📏 Model Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d728115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model calibration\n",
    "calibration_results = metrics_calc.calculate_calibration_metrics(y_true.tolist(), y_pred_proba, n_bins=10)\n",
    "\n",
    "print(\"📏 Model Calibration Analysis:\")\n",
    "\n",
    "# Overall calibration plot\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# For multiclass, we'll look at calibration for each class\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\"Calibration Curve (Class 0)\", \"Calibration Curve (Class 1)\",\n",
    "                   \"Reliability Diagram\", \"ECE by Class\"]\n",
    ")\n",
    "\n",
    "# Calibration curves for first two classes\n",
    "for class_idx, (row, col) in enumerate([(1, 1), (1, 2)]):\n",
    "    if class_idx < 2:\n",
    "        y_binary = (y_true == class_idx).astype(int)\n",
    "        prob_class = y_pred_proba[:, class_idx]\n",
    "        \n",
    "        if len(np.unique(y_binary)) == 2:  # Only if both classes present\n",
    "            fraction_pos, mean_pred_value = calibration_curve(y_binary, prob_class, n_bins=10)\n",
    "            \n",
    "            # Perfect calibration line\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[0, 1], y=[0, 1], mode='lines', \n",
    "                          name='Perfect Calibration', line=dict(dash='dash', color='gray')),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            \n",
    "            # Actual calibration\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=mean_pred_value, y=fraction_pos, mode='lines+markers',\n",
    "                          name=f'Class {class_idx} Calibration'),\n",
    "                row=row, col=col\n",
    "            )\n",
    "\n",
    "# Reliability diagram (confidence vs accuracy)\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "bin_accuracies = []\n",
    "bin_confidences = []\n",
    "bin_counts = []\n",
    "\n",
    "max_probs = np.max(y_pred_proba, axis=1)\n",
    "predictions = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "for i in range(len(confidence_bins) - 1):\n",
    "    mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i + 1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc = (predictions[mask] == y_true[mask]).mean()\n",
    "        bin_conf = max_probs[mask].mean()\n",
    "        bin_accuracies.append(bin_acc)\n",
    "        bin_confidences.append(bin_conf)\n",
    "        bin_counts.append(mask.sum())\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "        bin_confidences.append(bin_centers[i])\n",
    "        bin_counts.append(0)\n",
    "\n",
    "# Perfect calibration line\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0, 1], y=[0, 1], mode='lines', \n",
    "              name='Perfect Calibration', line=dict(dash='dash', color='gray')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Actual reliability\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=bin_confidences, y=bin_accuracies, mode='lines+markers',\n",
    "              name='Model Calibration', marker=dict(size=[c/10 for c in bin_counts])),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# ECE by class (if available)\n",
    "if calibration_results and 'error' not in calibration_results:\n",
    "    ece_scores = []\n",
    "    class_names = []\n",
    "    for key, value in calibration_results.items():\n",
    "        if isinstance(value, dict) and 'ece' in value:\n",
    "            ece_scores.append(value['ece'])\n",
    "            class_names.append(key)\n",
    "    \n",
    "    if ece_scores:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=class_names, y=ece_scores, name=\"ECE by Class\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"📏 Model Calibration Analysis\")\n",
    "fig.show()\n",
    "\n",
    "# Calculate overall ECE\n",
    "ece_overall = sum(abs(acc - conf) * count for acc, conf, count in zip(bin_accuracies, bin_confidences, bin_counts)) / sum(bin_counts)\n",
    "print(f\"\\nOverall Expected Calibration Error (ECE): {ece_overall:.4f}\")\n",
    "print(f\"Average confidence: {max_probs.mean():.3f}\")\n",
    "print(f\"Actual accuracy: {(predictions == y_true).mean():.3f}\")\n",
    "print(f\"Calibration gap: {max_probs.mean() - (predictions == y_true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bf6dd",
   "metadata": {},
   "source": [
    "## 📄 Final Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive final report\n",
    "final_report = {\n",
    "    \"model_info\": {\n",
    "        \"architecture\": \"Multimodal Transformer\",\n",
    "        \"visual_encoder\": \"ViT-Base/16\",\n",
    "        \"text_encoder\": \"BERT-base-uncased\",\n",
    "        \"fusion_mechanism\": \"Cross-modal Attention\"\n",
    "    },\n",
    "    \"training_results\": {\n",
    "        \"best_val_accuracy\": float(best_val_acc),\n",
    "        \"best_epoch\": int(best_val_epoch),\n",
    "        \"final_train_accuracy\": float(results_df['train_accuracy'].iloc[-1]),\n",
    "        \"final_val_accuracy\": float(results_df['val_accuracy'].iloc[-1]),\n",
    "        \"final_train_loss\": float(results_df['train_loss'].iloc[-1]),\n",
    "        \"final_val_loss\": float(results_df['val_loss'].iloc[-1]),\n",
    "        \"overfitting_score\": float(accuracy_gap.iloc[-1])\n",
    "    },\n",
    "    \"test_performance\": {\n",
    "        \"accuracy\": float(basic_metrics['accuracy']),\n",
    "        \"precision_macro\": float(basic_metrics['precision_macro']),\n",
    "        \"recall_macro\": float(basic_metrics['recall_macro']),\n",
    "        \"f1_macro\": float(basic_metrics['f1_macro']),\n",
    "        \"top1_accuracy\": float(top1_acc),\n",
    "        \"top3_accuracy\": float(top3_acc),\n",
    "        \"top5_accuracy\": float(top5_acc)\n",
    "    },\n",
    "    \"advanced_metrics\": {\n",
    "        \"mean_confidence\": float(advanced_metrics['mean_confidence']),\n",
    "        \"cross_entropy_loss\": float(advanced_metrics['cross_entropy']),\n",
    "        \"expected_calibration_error\": float(ece_overall)\n",
    "    },\n",
    "    \"error_analysis\": {\n",
    "        \"total_errors\": int(top_k_errors['total_incorrect']),\n",
    "        \"top5_recovery_rate\": float(top_k_errors['top_k_recovery_rate']),\n",
    "        \"confidence_gap\": float(correct_confidences.mean() - error_confidences.mean()),\n",
    "        \"most_confused_classes\": [int(most_confused[0]), int(most_confused[1])]\n",
    "    },\n",
    "    \"feature_analysis\": {\n",
    "        \"pca_variance_explained\": float(pca.explained_variance_ratio_.sum()),\n",
    "        \"average_silhouette_score\": float(np.mean(silhouette_scores)),\n",
    "        \"feature_dimension\": int(feature_dim)\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"num_classes\": int(num_classes),\n",
    "        \"num_samples\": int(num_samples),\n",
    "        \"class_imbalance_ratio\": float(balance_metrics['imbalance_ratio']),\n",
    "        \"gini_coefficient\": float(balance_metrics['gini_coefficient'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print executive summary\n",
    "print(\"📄 MULTIMODAL PILL RECOGNITION - FINAL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n🎯 EXECUTIVE SUMMARY:\")\n",
    "print(f\"The multimodal pill recognition model achieved {final_report['test_performance']['accuracy']:.1%} accuracy\")\n",
    "print(f\"on the test set, with {final_report['test_performance']['top5_accuracy']:.1%} top-5 accuracy.\")\n",
    "print(f\"The model shows good calibration (ECE: {final_report['advanced_metrics']['expected_calibration_error']:.3f})\")\n",
    "print(f\"and strong feature representations (Silhouette: {final_report['feature_analysis']['average_silhouette_score']:.3f}).\")\n",
    "\n",
    "print(\"\\n🏆 KEY ACHIEVEMENTS:\")\n",
    "print(f\"✅ High accuracy: {final_report['test_performance']['accuracy']:.1%}\")\n",
    "print(f\"✅ Excellent top-5 performance: {final_report['test_performance']['top5_accuracy']:.1%}\")\n",
    "print(f\"✅ Balanced precision/recall: {final_report['test_performance']['f1_macro']:.3f} F1-score\")\n",
    "print(f\"✅ Well-calibrated predictions: {final_report['advanced_metrics']['expected_calibration_error']:.3f} ECE\")\n",
    "print(f\"✅ Successful multimodal fusion: Cross-attention mechanism\")\n",
    "\n",
    "print(\"\\n⚠️ AREAS FOR IMPROVEMENT:\")\n",
    "if final_report['training_results']['overfitting_score'] > 0.05:\n",
    "    print(f\"- Overfitting detected: {final_report['training_results']['overfitting_score']:.3f} gap\")\n",
    "if final_report['advanced_metrics']['expected_calibration_error'] > 0.1:\n",
    "    print(f\"- Calibration could be improved: {final_report['advanced_metrics']['expected_calibration_error']:.3f} ECE\")\n",
    "if final_report['dataset_info']['class_imbalance_ratio'] > 10:\n",
    "    print(f\"- Class imbalance: {final_report['dataset_info']['class_imbalance_ratio']:.1f}:1 ratio\")\n",
    "\n",
    "print(\"\\n🚀 RECOMMENDATIONS:\")\n",
    "print(\"1. Deploy model for production use with confidence thresholding\")\n",
    "print(\"2. Collect more data for underrepresented classes\")\n",
    "print(\"3. Implement active learning for continuous improvement\")\n",
    "print(\"4. Consider ensemble methods for critical applications\")\n",
    "print(\"5. Monitor model performance and retrain periodically\")\n",
    "\n",
    "# Save detailed report\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "with open('../results/final_model_report.json', 'w') as f:\n",
    "    json.dump(final_report, f, indent=2)\n",
    "\n",
    "# Create a summary table\n",
    "summary_df = pd.DataFrame([\n",
    "    [\"Accuracy\", f\"{final_report['test_performance']['accuracy']:.1%}\"],\n",
    "    [\"Top-5 Accuracy\", f\"{final_report['test_performance']['top5_accuracy']:.1%}\"],\n",
    "    [\"F1-Score (Macro)\", f\"{final_report['test_performance']['f1_macro']:.3f}\"],\n",
    "    [\"Calibration (ECE)\", f\"{final_report['advanced_metrics']['expected_calibration_error']:.3f}\"],\n",
    "    [\"Mean Confidence\", f\"{final_report['advanced_metrics']['mean_confidence']:.3f}\"],\n",
    "    [\"Cross-Entropy Loss\", f\"{final_report['advanced_metrics']['cross_entropy_loss']:.3f}\"],\n",
    "    [\"Feature Quality (Silhouette)\", f\"{final_report['feature_analysis']['average_silhouette_score']:.3f}\"],\n",
    "    [\"PCA Variance Explained\", f\"{final_report['feature_analysis']['pca_variance_explained']:.1%}\"]\n",
    "], columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "print(\"\\n📊 PERFORMANCE SUMMARY:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n📄 Detailed report saved to: results/final_model_report.json\")\n",
    "print(\"\\n🎉 Analysis Complete! Ready for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
