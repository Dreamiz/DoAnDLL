{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c555ac",
   "metadata": {},
   "source": [
    "# üíä Demo: H·ªá th·ªëng Nh·∫≠n d·∫°ng Vi√™n Thu·ªëc Multimodal v·ªõi Transformer\n",
    "\n",
    "## T·ªïng quan\n",
    "Notebook n√†y minh h·ªça quy tr√¨nh ho√†n ch·ªânh ƒë·ªÉ x√¢y d·ª±ng v√† ƒë√°nh gi√° h·ªá th·ªëng nh·∫≠n d·∫°ng vi√™n thu·ªëc s·ª≠ d·ª•ng:\n",
    "\n",
    "- **Multimodal Transformer**: K·∫øt h·ª£p th√¥ng tin t·ª´ h√¨nh ·∫£nh v√† text imprint\n",
    "- **Cross-modal Attention**: H·ªçc representation chung cho visual v√† textual features  \n",
    "- **Apache Spark**: X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn\n",
    "- **GPU Acceleration**: Rapids cuDF/cuML ƒë·ªÉ tƒÉng t·ªëc\n",
    "\n",
    "## M·ª•c ti√™u\n",
    "1. X·ª≠ l√Ω d·ªØ li·ªáu multimodal (h√¨nh ·∫£nh + text)\n",
    "2. X√¢y d·ª±ng v√† training Multimodal Transformer\n",
    "3. ƒê√°nh gi√° hi·ªáu su·∫•t model\n",
    "4. Demo inference tr√™n d·ªØ li·ªáu th·ª±c t·∫ø"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389f2fb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho x·ª≠ l√Ω d·ªØ li·ªáu, machine learning v√† visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import timm\n",
    "\n",
    "# Spark v√† Big Data\n",
    "try:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, udf\n",
    "    from pyspark.sql.types import StringType, BinaryType, ArrayType, FloatType\n",
    "    SPARK_AVAILABLE = True\n",
    "    print(\"‚úÖ Apache Spark available\")\n",
    "except ImportError:\n",
    "    SPARK_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Apache Spark not available\")\n",
    "\n",
    "# Rapids for GPU acceleration\n",
    "try:\n",
    "    import cudf\n",
    "    import cupy as cp\n",
    "    RAPIDS_AVAILABLE = True\n",
    "    print(\"‚úÖ Rapids CUDF/CuPy available\")\n",
    "except ImportError:\n",
    "    RAPIDS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Rapids not available, using CPU\")\n",
    "\n",
    "# Plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Custom modules\n",
    "sys.path.append('../src')\n",
    "from models.multimodal_transformer import MultimodalPillTransformer\n",
    "from data.data_processing import SparkDataProcessor, PillDataset, get_data_transforms\n",
    "from utils.metrics import MetricsCalculator\n",
    "from utils.utils import set_seed, get_device\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcb7f5",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Data\n",
    "Thi·∫øt l·∫≠p c·∫•u h√¨nh v√† t·∫£i d·ªØ li·ªáu m·∫´u ƒë·ªÉ demo h·ªá th·ªëng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012da486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"visual_encoder\": {\n",
    "            \"type\": \"vit\",\n",
    "            \"model_name\": \"vit_base_patch16_224\",\n",
    "            \"pretrained\": True,\n",
    "            \"output_dim\": 768\n",
    "        },\n",
    "        \"text_encoder\": {\n",
    "            \"type\": \"bert\", \n",
    "            \"model_name\": \"bert-base-uncased\",\n",
    "            \"pretrained\": True,\n",
    "            \"output_dim\": 768,\n",
    "            \"max_length\": 128\n",
    "        },\n",
    "        \"fusion\": {\n",
    "            \"type\": \"cross_attention\",\n",
    "            \"hidden_dim\": 512,\n",
    "            \"num_attention_heads\": 8,\n",
    "            \"dropout\": 0.1\n",
    "        },\n",
    "        \"classifier\": {\n",
    "            \"num_classes\": 100,\n",
    "            \"hidden_dims\": [512, 256],\n",
    "            \"dropout\": 0.3\n",
    "        }\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"image_size\": 224,\n",
    "        \"spark\": {\n",
    "            \"app_name\": \"PillRecognitionDemo\",\n",
    "            \"master\": \"local[2]\",\n",
    "            \"executor_memory\": \"2g\",\n",
    "            \"driver_memory\": \"1g\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded:\")\n",
    "print(f\"  - Visual Encoder: {config['model']['visual_encoder']['model_name']}\")\n",
    "print(f\"  - Text Encoder: {config['model']['text_encoder']['model_name']}\")\n",
    "print(f\"  - Fusion Type: {config['model']['fusion']['type']}\")\n",
    "print(f\"  - Number of Classes: {config['model']['classifier']['num_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o d·ªØ li·ªáu m·∫´u cho demo\n",
    "def create_sample_data(num_samples=500):\n",
    "    \"\"\"T·∫°o d·ªØ li·ªáu m·∫´u v·ªõi h√¨nh ·∫£nh v√† text imprint\"\"\"\n",
    "    \n",
    "    # Danh s√°ch c√°c lo·∫°i thu·ªëc m·∫´u\n",
    "    pill_classes = [\n",
    "        \"Acetaminophen 500mg\", \"Ibuprofen 200mg\", \"Aspirin 325mg\",\n",
    "        \"Metformin 500mg\", \"Lisinopril 10mg\", \"Atorvastatin 20mg\",\n",
    "        \"Amlodipine 5mg\", \"Omeprazole 20mg\", \"Levothyroxine 50mcg\",\n",
    "        \"Simvastatin 40mg\", \"Losartan 50mg\", \"Gabapentin 300mg\"\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Random pill class\n",
    "        pill_class = pill_classes[i % len(pill_classes)]\n",
    "        class_id = i % len(pill_classes)\n",
    "        \n",
    "        # Generate synthetic image (224x224x3)\n",
    "        np.random.seed(i)\n",
    "        image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Add some pattern to make it look more realistic\n",
    "        center = (112, 112)\n",
    "        radius = np.random.randint(40, 80)\n",
    "        color = tuple(np.random.randint(100, 255, 3).tolist())\n",
    "        cv2.circle(image, center, radius, color, -1)\n",
    "        \n",
    "        # Generate text imprint patterns\n",
    "        text_patterns = [\n",
    "            f\"PILL{i:03d}\",\n",
    "            f\"{class_id}MG\",\n",
    "            f\"RX{i:02d}\",\n",
    "            f\"TAB{class_id}\",\n",
    "            f\"MED{i%100:02d}\"\n",
    "        ]\n",
    "        text_imprint = text_patterns[i % len(text_patterns)]\n",
    "        \n",
    "        data.append({\n",
    "            \"image_id\": f\"img_{i:05d}\",\n",
    "            \"image_array\": image,\n",
    "            \"text_imprint\": text_imprint,\n",
    "            \"pill_class\": pill_class,\n",
    "            \"class_id\": class_id,\n",
    "            \"manufacturer\": f\"Company_{chr(65 + i%5)}\",  # Company_A, B, C, D, E\n",
    "            \"dosage\": pill_class.split()[-1] if \"mg\" in pill_class else \"Unknown\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# T·∫°o dataset m·∫´u\n",
    "print(\"üîÑ Generating sample dataset...\")\n",
    "sample_df = create_sample_data(500)\n",
    "\n",
    "print(f\"‚úÖ Created dataset with {len(sample_df)} samples\")\n",
    "print(f\"üìä Number of unique classes: {sample_df['class_id'].nunique()}\")\n",
    "print(f\"üìù Sample text imprints: {sample_df['text_imprint'].unique()[:10]}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "print(sample_df.info())\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "sample_df[['image_id', 'text_imprint', 'pill_class', 'manufacturer']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55355416",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu h√¨nh ·∫£nh v√† text ƒë·ªÉ chu·∫©n b·ªã cho model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb72214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã m·ªôt s·ªë m·∫´u h√¨nh ·∫£nh\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle(\"üì∏ Sample Pill Images\", fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(10):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    \n",
    "    # Get image and info\n",
    "    image = sample_df.iloc[i]['image_array']\n",
    "    text = sample_df.iloc[i]['text_imprint']\n",
    "    pill_class = sample_df.iloc[i]['pill_class']\n",
    "    \n",
    "    # Display image\n",
    "    axes[row, col].imshow(image)\n",
    "    axes[row, col].set_title(f\"{text}\\n{pill_class[:15]}...\", fontsize=8)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ph√¢n b·ªë d·ªØ li·ªáu\n",
    "print(\"\\nüìä Class Distribution:\")\n",
    "class_counts = sample_df['pill_class'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Visualization\n",
    "fig = px.bar(\n",
    "    x=class_counts.index,\n",
    "    y=class_counts.values,\n",
    "    title=\"Ph√¢n b·ªë c√°c l·ªõp thu·ªëc trong dataset\",\n",
    "    labels={'x': 'Lo·∫°i thu·ªëc', 'y': 'S·ªë l∆∞·ª£ng m·∫´u'}\n",
    ")\n",
    "fig.update_xaxis(tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "# Text imprint analysis\n",
    "print(\"\\nüìù Text Imprint Analysis:\")\n",
    "text_lengths = sample_df['text_imprint'].str.len()\n",
    "print(f\"Text length - Min: {text_lengths.min()}, Max: {text_lengths.max()}, Mean: {text_lengths.mean():.2f}\")\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=text_lengths,\n",
    "    title=\"Ph√¢n b·ªë ƒë·ªô d√†i Text Imprint\",\n",
    "    labels={'x': 'ƒê·ªô d√†i text', 'y': 'S·ªë l∆∞·ª£ng'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10fc4b8",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "T·∫°o v√† x·ª≠ l√Ω features cho c·∫£ h√¨nh ·∫£nh v√† text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations cho training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(sample_df, test_size=0.3, random_state=42, stratify=sample_df['class_id'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['class_id'])\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"  - Training: {len(train_df)} samples\")\n",
    "print(f\"  - Validation: {len(val_df)} samples\") \n",
    "print(f\"  - Test: {len(test_df)} samples\")\n",
    "\n",
    "# Custom Dataset class\n",
    "class DemoPillDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Image\n",
    "        image = row['image_array']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "            \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': row['text_imprint'],\n",
    "            'label': row['class_id'],\n",
    "            'class_name': row['pill_class']\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DemoPillDataset(train_df, train_transform)\n",
    "val_dataset = DemoPillDataset(val_df, val_transform)\n",
    "test_dataset = DemoPillDataset(test_df, val_transform)\n",
    "\n",
    "print(\"‚úÖ Datasets created successfully!\")\n",
    "\n",
    "# Show sample after transformation\n",
    "sample_batch = train_dataset[0]\n",
    "print(f\"\\nüîç Sample after transformation:\")\n",
    "print(f\"  - Image shape: {sample_batch['image'].shape}\")\n",
    "print(f\"  - Text: '{sample_batch['text']}'\")\n",
    "print(f\"  - Label: {sample_batch['label']}\")\n",
    "print(f\"  - Class: {sample_batch['class_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd52275",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "Kh·ªüi t·∫°o v√† training Multimodal Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"ü§ñ Initializing Multimodal Transformer...\")\n",
    "model = MultimodalPillTransformer(config[\"model\"]).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üìä Model Statistics:\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Setup training\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for multimodal data\"\"\"\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "    class_names = [item['class_name'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'texts': texts,\n",
    "        'labels': labels,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"‚úÖ Training setup complete!\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Training function (simplified for demo)\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    tokenizer = model.get_text_tokenizer()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        images = batch['images'].to(device)\n",
    "        texts = batch['texts']\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Tokenize texts\n",
    "        text_inputs = tokenizer(\n",
    "            texts,\n",
    "            max_length=128,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, text_inputs)\n",
    "        loss = criterion(outputs[\"logits\"], labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs[\"logits\"], 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "# Quick training demo (just a few epochs)\n",
    "print(\"üöÄ Starting training demo (3 epochs)...\")\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/3: Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "print(\"‚úÖ Training demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bcb75d",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "ƒê√°nh gi√° hi·ªáu su·∫•t model tr√™n validation set v√† ph√¢n t√≠ch k·∫øt qu·∫£."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_features = {'visual': [], 'text': [], 'fused': []}\n",
    "    \n",
    "    tokenizer = model.get_text_tokenizer()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['images'].to(device)\n",
    "            texts = batch['texts']\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Tokenize texts\n",
    "            text_inputs = tokenizer(\n",
    "                texts,\n",
    "                max_length=128,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, text_inputs, return_features=True)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Store features for analysis\n",
    "            all_features['visual'].append(outputs[\"visual_features\"].cpu())\n",
    "            all_features['text'].append(outputs[\"text_features\"].cpu())\n",
    "            all_features['fused'].append(outputs[\"fused_features\"].cpu())\n",
    "    \n",
    "    # Concatenate features\n",
    "    for key in all_features:\n",
    "        all_features[key] = torch.cat(all_features[key], dim=0)\n",
    "    \n",
    "    return all_predictions, all_labels, all_features\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"üîç Evaluating model on validation set...\")\n",
    "val_predictions, val_labels, val_features = evaluate_model(model, val_loader, device)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"üìä Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "class_names = train_df['pill_class'].unique()[:len(set(val_labels))]\n",
    "report = classification_report(val_labels, val_predictions, target_names=class_names, output_dict=True)\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(val_labels, val_predictions)\n",
    "fig = px.imshow(\n",
    "    cm,\n",
    "    title=\"Confusion Matrix\",\n",
    "    labels=dict(x=\"Predicted\", y=\"Actual\"),\n",
    "    color_continuous_scale=\"Blues\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Feature analysis\n",
    "print(\"\\nüîç Feature Analysis:\")\n",
    "visual_magnitude = torch.norm(val_features['visual'], dim=1).mean().item()\n",
    "text_magnitude = torch.norm(val_features['text'], dim=1).mean().item()\n",
    "fused_magnitude = torch.norm(val_features['fused'], dim=1).mean().item()\n",
    "\n",
    "print(f\"  - Visual features magnitude: {visual_magnitude:.4f}\")\n",
    "print(f\"  - Text features magnitude: {text_magnitude:.4f}\")\n",
    "print(f\"  - Fused features magnitude: {fused_magnitude:.4f}\")\n",
    "\n",
    "# Cross-modal similarity\n",
    "visual_norm = torch.nn.functional.normalize(val_features['visual'], dim=1)\n",
    "text_norm = torch.nn.functional.normalize(val_features['text'], dim=1)\n",
    "similarity = torch.mean(torch.sum(visual_norm * text_norm, dim=1)).item()\n",
    "print(f\"  - Visual-Text similarity: {similarity:.4f}\")\n",
    "\n",
    "# Training progress visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Training Loss', 'Training Accuracy')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(1, len(train_losses)+1)), y=train_losses, name='Loss'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(1, len(train_accs)+1)), y=train_accs, name='Accuracy'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text=\"Training Progress\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816af32",
   "metadata": {},
   "source": [
    "## 7. Make Predictions\n",
    "Demo inference tr√™n d·ªØ li·ªáu m·ªõi v√† ph√¢n t√≠ch k·∫øt qu·∫£ d·ª± ƒëo√°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24142bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_pill(model, image, text_imprint, device, top_k=5):\n",
    "    \"\"\"Make prediction on a single pill image and text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = model.get_text_tokenizer()\n",
    "    \n",
    "    # Preprocess image\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = val_transform(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Tokenize text\n",
    "    text_inputs = tokenizer(\n",
    "        [text_imprint],\n",
    "        max_length=128,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image, text_inputs, return_features=True)\n",
    "        probs = torch.softmax(outputs[\"logits\"], dim=1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=top_k, dim=1)\n",
    "    \n",
    "    results = {\n",
    "        'top_classes': top_indices[0].cpu().numpy(),\n",
    "        'top_probabilities': top_probs[0].cpu().numpy(),\n",
    "        'features': {\n",
    "            'visual': outputs[\"visual_features\"].cpu(),\n",
    "            'text': outputs[\"text_features\"].cpu(),\n",
    "            'fused': outputs[\"fused_features\"].cpu()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Demo predictions on test samples\n",
    "print(\"üîÆ Making predictions on test samples...\")\n",
    "\n",
    "# Select a few test samples\n",
    "test_indices = [0, 5, 10, 15, 20]\n",
    "fig, axes = plt.subplots(1, len(test_indices), figsize=(20, 4))\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    # Get test sample\n",
    "    sample = test_dataset[idx]\n",
    "    image = sample['image']\n",
    "    text = sample['text']\n",
    "    true_label = sample['label']\n",
    "    true_class = sample['class_name']\n",
    "    \n",
    "    # Make prediction\n",
    "    results = predict_pill(model, image, text, device)\n",
    "    \n",
    "    # Get original image for display\n",
    "    original_image = test_df.iloc[idx]['image_array']\n",
    "    \n",
    "    # Display\n",
    "    axes[i].imshow(original_image)\n",
    "    axes[i].set_title(\n",
    "        f\"Text: {text}\\\\n\"\n",
    "        f\"True: {true_class[:15]}...\\\\n\"\n",
    "        f\"Pred: Class {results['top_classes'][0]}\\\\n\"\n",
    "        f\"Conf: {results['top_probabilities'][0]:.3f}\",\n",
    "        fontsize=10\n",
    "    )\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    print(f\"\\\\nüìã Sample {idx}:\")\n",
    "    print(f\"  Text: '{text}'\")\n",
    "    print(f\"  True class: {true_class}\")\n",
    "    print(f\"  Top 3 predictions:\")\n",
    "    for j in range(3):\n",
    "        class_id = results['top_classes'][j]\n",
    "        prob = results['top_probabilities'][j]\n",
    "        print(f\"    {j+1}. Class {class_id}: {prob:.3f}\")\n",
    "\n",
    "plt.suptitle(\"üîÆ Inference Results on Test Samples\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature visualization for multimodal analysis\n",
    "print(\"\\\\nüé® Multimodal Feature Analysis:\")\n",
    "\n",
    "# Get features from a batch of test samples\n",
    "test_loader_single = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Get one batch for analysis\n",
    "test_batch = next(iter(test_loader_single))\n",
    "test_predictions, test_labels_batch, test_features_batch = evaluate_model(\n",
    "    model, [test_batch], device\n",
    ")\n",
    "\n",
    "# Plot feature distributions\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Visual Features', 'Text Features', 'Fused Features')\n",
    ")\n",
    "\n",
    "for i, (feature_type, features) in enumerate(test_features_batch.items()):\n",
    "    feature_norms = torch.norm(features, dim=1).numpy()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=feature_norms, name=f'{feature_type.title()} Features'),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Feature Magnitude Distributions\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"‚úÖ Inference demo completed!\")\n",
    "print(\"\\\\nüéâ Notebook demo ho√†n th√†nh! H·ªá th·ªëng multimodal pill recognition ƒë√£ ƒë∆∞·ª£c demo th√†nh c√¥ng.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
