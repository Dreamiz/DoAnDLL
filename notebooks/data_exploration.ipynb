{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c3c4bf",
   "metadata": {},
   "source": [
    "# üìä Data Exploration & Analysis\n",
    "\n",
    "This notebook provides comprehensive data exploration and analysis for the multimodal pill recognition dataset.\n",
    "\n",
    "## üéØ Objectives\n",
    "- Analyze dataset statistics and distributions\n",
    "- Visualize image and text data characteristics\n",
    "- Identify data quality issues\n",
    "- Generate insights for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed26d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import base64\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from data.data_processing import SparkDataProcessor\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879abfc",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d617d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"- Data path: {config['data']['data_path']}\")\n",
    "print(f\"- Image size: {config['data']['image_size']}\")\n",
    "print(f\"- Number of classes: {config['model']['classifier']['num_classes']}\")\n",
    "\n",
    "# Initialize Spark data processor\n",
    "try:\n",
    "    processor = SparkDataProcessor(config)\n",
    "    print(\"‚úÖ Spark processor initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not initialize Spark: {e}\")\n",
    "    processor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3568fd",
   "metadata": {},
   "source": [
    "## üé≤ Sample Data Generation\n",
    "\n",
    "Generate sample synthetic data for exploration and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if processor:\n",
    "    # Generate sample dataset\n",
    "    sample_size = 1000\n",
    "    output_path = \"../data/raw/sample_data.parquet\"\n",
    "    \n",
    "    print(f\"üé≤ Generating {sample_size} sample records...\")\n",
    "    sample_df = processor.create_sample_dataset(output_path, sample_size)\n",
    "    \n",
    "    # Convert to Pandas for analysis\n",
    "    sample_pd = sample_df.toPandas()\n",
    "    print(f\"‚úÖ Sample data generated: {len(sample_pd)} records\")\n",
    "else:\n",
    "    # Create dummy data without Spark\n",
    "    print(\"üìù Creating dummy data without Spark...\")\n",
    "    \n",
    "    pill_classes = [f\"pill_class_{i:04d}\" for i in range(50)]\n",
    "    manufacturers = [f\"pharma_company_{i}\" for i in range(1, 11)]\n",
    "    shapes = [\"round\", \"oval\", \"square\", \"capsule\"]\n",
    "    colors = [\"white\", \"blue\", \"red\", \"yellow\", \"green\", \"pink\", \"orange\"]\n",
    "    \n",
    "    sample_data = []\n",
    "    for i in range(1000):\n",
    "        sample_data.append({\n",
    "            \"id\": f\"pill_{i:06d}\",\n",
    "            \"pill_class\": np.random.choice(pill_classes),\n",
    "            \"manufacturer\": np.random.choice(manufacturers),\n",
    "            \"dosage\": f\"{np.random.randint(5, 500)}mg\",\n",
    "            \"shape\": np.random.choice(shapes),\n",
    "            \"color\": np.random.choice(colors),\n",
    "            \"text_imprint\": f\"PILL {np.random.randint(1, 999)}\",\n",
    "            \"split\": np.random.choice([\"train\", \"val\", \"test\"], p=[0.7, 0.15, 0.15])\n",
    "        })\n",
    "    \n",
    "    sample_pd = pd.DataFrame(sample_data)\n",
    "    print(f\"‚úÖ Dummy data created: {len(sample_pd)} records\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(sample_pd.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d6d43",
   "metadata": {},
   "source": [
    "## üìà Basic Statistics & Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e25f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"üìã Dataset Statistics:\")\n",
    "print(f\"Total samples: {len(sample_pd):,}\")\n",
    "print(f\"Unique classes: {sample_pd['pill_class'].nunique()}\")\n",
    "print(f\"Unique manufacturers: {sample_pd['manufacturer'].nunique()}\")\n",
    "print(f\"Unique shapes: {sample_pd['shape'].nunique()}\")\n",
    "print(f\"Unique colors: {sample_pd['color'].nunique()}\")\n",
    "\n",
    "# Train/Val/Test split\n",
    "print(\"\\nüéØ Data Split:\")\n",
    "split_counts = sample_pd['split'].value_counts()\n",
    "for split, count in split_counts.items():\n",
    "    percentage = (count / len(sample_pd)) * 100\n",
    "    print(f\"{split}: {count:,} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ab0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution analysis\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\"Class Distribution\", \"Manufacturer Distribution\", \n",
    "                   \"Shape Distribution\", \"Color Distribution\"],\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"pie\"}, {\"type\": \"pie\"}]]\n",
    ")\n",
    "\n",
    "# Class distribution (top 20)\n",
    "top_classes = sample_pd['pill_class'].value_counts().head(20)\n",
    "fig.add_trace(\n",
    "    go.Bar(x=top_classes.index, y=top_classes.values, name=\"Classes\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Manufacturer distribution\n",
    "mfg_counts = sample_pd['manufacturer'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=mfg_counts.index, y=mfg_counts.values, name=\"Manufacturers\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Shape distribution\n",
    "shape_counts = sample_pd['shape'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=shape_counts.index, values=shape_counts.values, name=\"Shapes\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Color distribution\n",
    "color_counts = sample_pd['color'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=color_counts.index, values=color_counts.values, name=\"Colors\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"üìä Dataset Distribution Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42549430",
   "metadata": {},
   "source": [
    "## üìù Text Imprint Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d080c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text imprint analysis\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Analyze text lengths\n",
    "text_lengths = sample_pd['text_imprint'].str.len()\n",
    "word_counts = sample_pd['text_imprint'].str.split().str.len()\n",
    "\n",
    "print(\"üìù Text Imprint Statistics:\")\n",
    "print(f\"Average character length: {text_lengths.mean():.1f}\")\n",
    "print(f\"Average word count: {word_counts.mean():.1f}\")\n",
    "print(f\"Max character length: {text_lengths.max()}\")\n",
    "print(f\"Max word count: {word_counts.max()}\")\n",
    "\n",
    "# Extract all words and analyze frequency\n",
    "all_words = []\n",
    "for text in sample_pd['text_imprint']:\n",
    "    words = re.findall(r'\\b\\w+\\b', text.upper())\n",
    "    all_words.extend(words)\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"\\nüî§ Vocabulary size: {len(word_freq)}\")\n",
    "print(\"\\nüìä Most common words:\")\n",
    "for word, count in word_freq.most_common(10):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distributions\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"Character Length Distribution\", \"Word Count Distribution\"]\n",
    ")\n",
    "\n",
    "# Character length histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=text_lengths, nbinsx=20, name=\"Character Length\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Word count histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=word_counts, nbinsx=10, name=\"Word Count\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"üìù Text Imprint Length Analysis\")\n",
    "fig.show()\n",
    "\n",
    "# Word frequency chart\n",
    "top_words = dict(word_freq.most_common(20))\n",
    "fig_words = px.bar(\n",
    "    x=list(top_words.keys()),\n",
    "    y=list(top_words.values()),\n",
    "    title=\"üî§ Top 20 Most Frequent Words in Text Imprints\",\n",
    "    labels={\"x\": \"Words\", \"y\": \"Frequency\"}\n",
    ")\n",
    "fig_words.update_xaxis(tickangle=45)\n",
    "fig_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2448973",
   "metadata": {},
   "source": [
    "## üîç Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe311e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"üîç Missing Value Analysis:\")\n",
    "missing_data = sample_pd.isnull().sum()\n",
    "for column, missing_count in missing_data.items():\n",
    "    if missing_count > 0:\n",
    "        percentage = (missing_count / len(sample_pd)) * 100\n",
    "        print(f\"  {column}: {missing_count} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"  {column}: No missing values ‚úÖ\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = sample_pd.duplicated().sum()\n",
    "print(f\"\\nüîÑ Duplicate records: {duplicates}\")\n",
    "\n",
    "# Check class balance\n",
    "print(\"\\n‚öñÔ∏è Class Balance Analysis:\")\n",
    "class_counts = sample_pd['pill_class'].value_counts()\n",
    "print(f\"Most frequent class: {class_counts.iloc[0]} samples\")\n",
    "print(f\"Least frequent class: {class_counts.iloc[-1]} samples\")\n",
    "print(f\"Imbalance ratio: {class_counts.iloc[0] / class_counts.iloc[-1]:.2f}\")\n",
    "\n",
    "# Calculate Gini coefficient for class imbalance\n",
    "def gini_coefficient(x):\n",
    "    sorted_x = sorted(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n + 1)\n",
    "    return (2 * np.sum(index * sorted_x)) / (n * np.sum(sorted_x)) - (n + 1) / n\n",
    "\n",
    "gini = gini_coefficient(class_counts.values)\n",
    "print(f\"Class distribution Gini coefficient: {gini:.3f}\")\n",
    "print(\"  (0 = perfectly balanced, 1 = maximally imbalanced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86951e41",
   "metadata": {},
   "source": [
    "## üîó Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52182d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between categorical features\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "categorical_features = ['manufacturer', 'shape', 'color']\n",
    "\n",
    "print(\"üîó Feature Association Analysis (Chi-square test):\")\n",
    "for i, feat1 in enumerate(categorical_features):\n",
    "    for feat2 in categorical_features[i+1:]:\n",
    "        # Create contingency table\n",
    "        contingency_table = pd.crosstab(sample_pd[feat1], sample_pd[feat2])\n",
    "        \n",
    "        # Perform chi-square test\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        # Calculate Cram√©r's V (effect size)\n",
    "        n = contingency_table.sum().sum()\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "        \n",
    "        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "        \n",
    "        print(f\"  {feat1} vs {feat2}:\")\n",
    "        print(f\"    Chi2: {chi2:.3f}, p-value: {p_value:.3f} {significance}\")\n",
    "        print(f\"    Cram√©r's V: {cramers_v:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-tabulations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\"Shape vs Color\", \"Manufacturer vs Shape\", \n",
    "                   \"Manufacturer vs Color\", \"Text Length vs Shape\"]\n",
    ")\n",
    "\n",
    "# Shape vs Color heatmap\n",
    "shape_color_crosstab = pd.crosstab(sample_pd['shape'], sample_pd['color'])\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=shape_color_crosstab.values,\n",
    "               x=shape_color_crosstab.columns,\n",
    "               y=shape_color_crosstab.index,\n",
    "               colorscale='Viridis'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Manufacturer vs Shape\n",
    "mfg_shape_crosstab = pd.crosstab(sample_pd['manufacturer'], sample_pd['shape'])\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=mfg_shape_crosstab.values,\n",
    "               x=mfg_shape_crosstab.columns,\n",
    "               y=mfg_shape_crosstab.index,\n",
    "               colorscale='Plasma'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Add text length by shape analysis\n",
    "text_len_by_shape = sample_pd.groupby('shape')['text_imprint'].str.len().mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=text_len_by_shape.index, y=text_len_by_shape.values),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"üîó Feature Relationships Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f239e62",
   "metadata": {},
   "source": [
    "## üí° Key Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate automated insights\n",
    "insights = []\n",
    "\n",
    "# Class balance insight\n",
    "if gini > 0.3:\n",
    "    insights.append(\"‚öñÔ∏è **Class Imbalance Detected**: Consider using class weights or sampling techniques during training.\")\n",
    "else:\n",
    "    insights.append(\"‚úÖ **Balanced Classes**: Dataset shows good class distribution.\")\n",
    "\n",
    "# Text length insight\n",
    "avg_text_len = text_lengths.mean()\n",
    "if avg_text_len < 10:\n",
    "    insights.append(\"üìù **Short Text Imprints**: Consider data augmentation or synthetic text generation.\")\n",
    "elif avg_text_len > 50:\n",
    "    insights.append(\"üìù **Long Text Imprints**: May need to increase maximum sequence length in model.\")\n",
    "else:\n",
    "    insights.append(\"‚úÖ **Optimal Text Length**: Text imprints are within good range for processing.\")\n",
    "\n",
    "# Vocabulary insight\n",
    "vocab_size = len(word_freq)\n",
    "if vocab_size < 100:\n",
    "    insights.append(\"üî§ **Limited Vocabulary**: Consider expanding text data or using character-level encoding.\")\n",
    "else:\n",
    "    insights.append(f\"‚úÖ **Rich Vocabulary**: {vocab_size} unique words provide good textual diversity.\")\n",
    "\n",
    "# Feature diversity insight\n",
    "shape_diversity = sample_pd['shape'].nunique()\n",
    "color_diversity = sample_pd['color'].nunique()\n",
    "insights.append(f\"üé® **Visual Diversity**: {shape_diversity} shapes and {color_diversity} colors provide good visual variation.\")\n",
    "\n",
    "# Data quality insight\n",
    "if duplicates == 0 and missing_data.sum() == 0:\n",
    "    insights.append(\"‚úÖ **High Data Quality**: No missing values or duplicates detected.\")\n",
    "else:\n",
    "    insights.append(\"‚ö†Ô∏è **Data Quality Issues**: Address missing values and duplicates before training.\")\n",
    "\n",
    "print(\"üí° Key Insights & Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "    print()\n",
    "\n",
    "# Save insights to file\n",
    "with open('../results/data_exploration_insights.txt', 'w') as f:\n",
    "    f.write(\"Data Exploration Insights\\n\")\n",
    "    f.write(\"=\" * 30 + \"\\n\\n\")\n",
    "    for insight in insights:\n",
    "        f.write(f\"- {insight}\\n\")\n",
    "\n",
    "print(\"üìÑ Insights saved to 'results/data_exploration_insights.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53fa50",
   "metadata": {},
   "source": [
    "## üìã Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "summary_report = {\n",
    "    \"dataset_overview\": {\n",
    "        \"total_samples\": len(sample_pd),\n",
    "        \"unique_classes\": sample_pd['pill_class'].nunique(),\n",
    "        \"unique_manufacturers\": sample_pd['manufacturer'].nunique(),\n",
    "        \"data_splits\": sample_pd['split'].value_counts().to_dict()\n",
    "    },\n",
    "    \"text_analysis\": {\n",
    "        \"vocabulary_size\": len(word_freq),\n",
    "        \"avg_text_length\": float(text_lengths.mean()),\n",
    "        \"avg_word_count\": float(word_counts.mean()),\n",
    "        \"top_words\": dict(word_freq.most_common(10))\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"missing_values\": missing_data.sum().to_dict(),\n",
    "        \"duplicate_records\": int(duplicates),\n",
    "        \"class_imbalance_gini\": float(gini)\n",
    "    },\n",
    "    \"feature_diversity\": {\n",
    "        \"shapes\": sample_pd['shape'].value_counts().to_dict(),\n",
    "        \"colors\": sample_pd['color'].value_counts().to_dict(),\n",
    "        \"manufacturers\": sample_pd['manufacturer'].value_counts().to_dict()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary as JSON\n",
    "import json\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "with open('../results/data_exploration_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\"üìä Data Exploration Complete!\")\n",
    "print(\"üìÑ Summary report saved to 'results/data_exploration_summary.json'\")\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"1. Review insights and recommendations\")\n",
    "print(\"2. Address any data quality issues\")\n",
    "print(\"3. Proceed with model training\")\n",
    "print(\"4. Monitor performance across different classes and features\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
