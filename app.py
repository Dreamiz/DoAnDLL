import streamlit as st
import torch
import numpy as np
from PIL import Image
import cv2
import yaml
import os
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
from streamlit_option_menu import option_menu
import io
import base64
from typing import Dict, Any, List, Tuple
import time

# Import custom modules
import sys
sys.path.append('src')

from models.multimodal_transformer import MultimodalPillTransformer
from data.data_processing import PillDataset, get_data_transforms
from utils.utils import load_checkpoint, get_device
from utils.metrics import MetricsCalculator

# Configure page
st.set_page_config(
    page_title="H·ªá th·ªëng Nh·∫≠n d·∫°ng Vi√™n Thu·ªëc Multimodal",
    page_icon="üíä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    .section-header {
        font-size: 1.5rem;
        color: #2e86c1;
        margin-top: 2rem;
        margin-bottom: 1rem;
        border-bottom: 2px solid #2e86c1;
        padding-bottom: 0.5rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .prediction-result {
        background-color: #e8f5e8;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 2px solid #28a745;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #ffeaa7;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def load_model_and_config():
    """Load model and configuration"""
    try:
        # Load configuration
        with open("config/config.yaml", "r") as f:
            config = yaml.safe_load(f)
        
        # Initialize model
        device = get_device()
        model = MultimodalPillTransformer(config["model"])
        
        # Load checkpoint if available
        checkpoint_path = "checkpoints/best_model.pth"
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            st.success("‚úÖ Model loaded successfully!")
        else:
            st.warning("‚ö†Ô∏è No trained model found. Using random weights.")
        
        model.to(device)
        model.eval()
        
        return model, config, device
    
    except Exception as e:
        st.error(f"‚ùå Error loading model: {str(e)}")
        return None, None, None


@st.cache_data
def load_sample_data():
    """Load sample data for demonstration"""
    try:
        # Create dummy data for demo
        pill_classes = [
            "Acetaminophen 500mg", "Ibuprofen 200mg", "Aspirin 325mg",
            "Metformin 500mg", "Lisinopril 10mg", "Atorvastatin 20mg",
            "Amlodipine 5mg", "Omeprazole 20mg", "Levothyroxine 50mcg",
            "Simvastatin 40mg"
        ]
        
        sample_data = []
        for i, pill_class in enumerate(pill_classes):
            sample_data.append({
                "id": i,
                "name": pill_class,
                "imprint": f"PILL{i:03d}",
                "description": f"M√¥ t·∫£ chi ti·∫øt v·ªÅ {pill_class}",
                "confidence": np.random.uniform(0.85, 0.99)
            })
        
        return sample_data
    
    except Exception as e:
        st.error(f"Error loading sample data: {str(e)}")
        return []


def preprocess_image(image: Image.Image, target_size: int = 224) -> torch.Tensor:
    """Preprocess image for model input"""
    try:
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Resize image
        image = image.resize((target_size, target_size))
        
        # Convert to numpy array
        image_array = np.array(image, dtype=np.float32) / 255.0
        
        # Normalize using ImageNet stats
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image_array = (image_array - mean) / std
        
        # Convert to tensor and add batch dimension
        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).unsqueeze(0)
        
        return image_tensor
    
    except Exception as e:
        st.error(f"Error preprocessing image: {str(e)}")
        return None


def predict_pill(model, image_tensor: torch.Tensor, text_imprint: str, 
                device, tokenizer, sample_data: List[Dict]) -> Dict[str, Any]:
    """Make prediction on pill image and text"""
    try:
        with torch.no_grad():
            # Move image to device
            image_tensor = image_tensor.to(device)
            
            # Tokenize text
            text_inputs = tokenizer(
                [text_imprint],
                max_length=128,
                padding=True,
                truncation=True,
                return_tensors="pt"
            ).to(device)
            
            # Get model predictions
            outputs = model(image_tensor, text_inputs, return_features=True)
            
            # Get probabilities
            probs = torch.softmax(outputs["logits"], dim=1)
            top_probs, top_indices = torch.topk(probs, k=5, dim=1)
            
            # Format results
            predictions = []
            for i in range(5):
                idx = top_indices[0][i].item()
                confidence = top_probs[0][i].item()
                
                # Get corresponding pill info (use sample data for demo)
                if idx < len(sample_data):
                    pill_info = sample_data[idx]
                    predictions.append({
                        "rank": i + 1,
                        "class_id": idx,
                        "name": pill_info["name"],
                        "imprint": pill_info["imprint"],
                        "confidence": confidence,
                        "description": pill_info["description"]
                    })
            
            return {
                "predictions": predictions,
                "features": {
                    "visual": outputs["visual_features"],
                    "text": outputs["text_features"],
                    "fused": outputs["fused_features"]
                }
            }
    
    except Exception as e:
        st.error(f"Error during prediction: {str(e)}")
        return None


def display_prediction_results(results: Dict[str, Any]):
    """Display prediction results"""
    if not results or "predictions" not in results:
        st.error("‚ùå No prediction results to display")
        return
    
    st.markdown('<div class="section-header">üéØ K·∫øt qu·∫£ Nh·∫≠n d·∫°ng</div>', unsafe_allow_html=True)
    
    # Top prediction
    top_pred = results["predictions"][0]
    
    st.markdown(f"""
    <div class="prediction-result">
        <h3>üèÜ D·ª± ƒëo√°n ch√≠nh: {top_pred['name']}</h3>
        <p><strong>Text Imprint:</strong> {top_pred['imprint']}</p>
        <p><strong>ƒê·ªô tin c·∫≠y:</strong> {top_pred['confidence']:.2%}</p>
        <p><strong>M√¥ t·∫£:</strong> {top_pred['description']}</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Top 5 predictions
    st.markdown('<div class="section-header">üìä Top 5 D·ª± ƒëo√°n</div>', unsafe_allow_html=True)
    
    pred_df = pd.DataFrame([
        {
            "Th·ª© h·∫°ng": pred["rank"],
            "T√™n thu·ªëc": pred["name"],
            "Text Imprint": pred["imprint"],
            "ƒê·ªô tin c·∫≠y": f"{pred['confidence']:.2%}"
        }
        for pred in results["predictions"]
    ])
    
    st.dataframe(pred_df, use_container_width=True)
    
    # Confidence chart
    fig = px.bar(
        x=[pred["name"][:20] + "..." if len(pred["name"]) > 20 else pred["name"] 
           for pred in results["predictions"]],
        y=[pred["confidence"] for pred in results["predictions"]],
        title="ƒê·ªô tin c·∫≠y c√°c d·ª± ƒëo√°n h√†ng ƒë·∫ßu",
        labels={"x": "Lo·∫°i thu·ªëc", "y": "ƒê·ªô tin c·∫≠y"}
    )
    fig.update_layout(showlegend=False)
    st.plotly_chart(fig, use_container_width=True)


def display_feature_analysis(features: Dict[str, torch.Tensor]):
    """Display feature analysis"""
    st.markdown('<div class="section-header">üîç Ph√¢n t√≠ch Features</div>', unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        visual_magnitude = torch.norm(features["visual"], dim=1).item()
        st.markdown(f"""
        <div class="metric-card">
            <h4>üñºÔ∏è Visual Features</h4>
            <p>Magnitude: {visual_magnitude:.4f}</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        text_magnitude = torch.norm(features["text"], dim=1).item()
        st.markdown(f"""
        <div class="metric-card">
            <h4>üìù Text Features</h4>
            <p>Magnitude: {text_magnitude:.4f}</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        fused_magnitude = torch.norm(features["fused"], dim=1).item()
        st.markdown(f"""
        <div class="metric-card">
            <h4>üîó Fused Features</h4>
            <p>Magnitude: {fused_magnitude:.4f}</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Feature similarity
    visual_norm = torch.nn.functional.normalize(features["visual"], dim=1)
    text_norm = torch.nn.functional.normalize(features["text"], dim=1)
    similarity = torch.sum(visual_norm * text_norm, dim=1).item()
    
    st.markdown(f"""
    <div class="metric-card">
        <h4>ü§ù Visual-Text Similarity</h4>
        <p>Cosine Similarity: {similarity:.4f}</p>
    </div>
    """, unsafe_allow_html=True)


def main():
    """Main Streamlit application"""
    
    # Header
    st.markdown('<div class="main-header">üíä H·ªá th·ªëng Nh·∫≠n d·∫°ng Vi√™n Thu·ªëc Multimodal</div>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    <div style="text-align: center; margin-bottom: 2rem;">
        <p style="font-size: 1.2rem; color: #666;">
            S·ª≠ d·ª•ng Multimodal Transformer ƒë·ªÉ nh·∫≠n d·∫°ng vi√™n thu·ªëc t·ª´ h√¨nh ·∫£nh v√† text imprint
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar navigation
    with st.sidebar:
        selected = option_menu(
            "Menu ch√≠nh",
            ["üè† Trang ch·ªß", "üîç Nh·∫≠n d·∫°ng", "üìä Th·ªëng k√™", "‚ÑπÔ∏è Th√¥ng tin"],
            icons=['house', 'search', 'bar-chart', 'info-circle'],
            menu_icon="cast",
            default_index=0,
            styles={
                "container": {"padding": "0!important", "background-color": "#fafafa"},
                "icon": {"color": "orange", "font-size": "18px"},
                "nav-link": {"font-size": "16px", "text-align": "left", "margin": "0px", "--hover-color": "#eee"},
                "nav-link-selected": {"background-color": "#1f77b4"},
            }
        )
    
    # Load model and data
    model, config, device = load_model_and_config()
    sample_data = load_sample_data()
    
    if model is None:
        st.error("‚ùå Kh√¥ng th·ªÉ t·∫£i model. Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t.")
        return
    
    # Get tokenizer
    tokenizer = model.get_text_tokenizer()
    
    if selected == "üè† Trang ch·ªß":
        st.markdown('<div class="section-header">üè† Trang ch·ªß</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("""
            ### Gi·ªõi thi·ªáu h·ªá th·ªëng
            
            H·ªá th·ªëng nh·∫≠n d·∫°ng vi√™n thu·ªëc s·ª≠ d·ª•ng c√¥ng ngh·ªá **Multimodal Transformer** ti√™n ti·∫øn ƒë·ªÉ:
            
            - üñºÔ∏è **Ph√¢n t√≠ch h√¨nh ·∫£nh vi√™n thu·ªëc** s·ª≠ d·ª•ng Vision Transformer (ViT)
            - üìù **X·ª≠ l√Ω text imprint** tr√™n vi√™n thu·ªëc b·∫±ng BERT
            - üîó **K·∫øt h·ª£p th√¥ng tin** t·ª´ hai ngu·ªìn d·ªØ li·ªáu b·∫±ng Cross-modal Attention
            - ‚ö° **X·ª≠ l√Ω song song** v·ªõi Apache Spark v√† GPU acceleration
            
            ### T√≠nh nƒÉng ch√≠nh
            
            - ‚úÖ Nh·∫≠n d·∫°ng ch√≠nh x√°c cao v·ªõi ƒë·ªô tin c·∫≠y
            - ‚úÖ X·ª≠ l√Ω ƒë·ªìng th·ªùi h√¨nh ·∫£nh v√† text
            - ‚úÖ Giao di·ªán th√¢n thi·ªán v√† d·ªÖ s·ª≠ d·ª•ng
            - ‚úÖ H·ªó tr·ª£ batch processing cho d·ªØ li·ªáu l·ªõn
            """)
        
        with col2:
            st.markdown("""
            ### üìà Th·ªëng k√™ h·ªá th·ªëng
            """)
            
            if config:
                st.markdown(f"""
                <div class="metric-card">
                    <h4>üéØ S·ªë l·ªõp thu·ªëc</h4>
                    <p>{config["model"]["classifier"]["num_classes"]} l·ªõp</p>
                </div>
                """, unsafe_allow_html=True)
                
                st.markdown(f"""
                <div class="metric-card">
                    <h4>üñºÔ∏è K√≠ch th∆∞·ªõc ·∫£nh</h4>
                    <p>{config["data"]["image_size"]}x{config["data"]["image_size"]} pixels</p>
                </div>
                """, unsafe_allow_html=True)
                
                st.markdown(f"""
                <div class="metric-card">
                    <h4>üìù ƒê·ªô d√†i text t·ªëi ƒëa</h4>
                    <p>{config["model"]["text_encoder"]["max_length"]} tokens</p>
                </div>
                """, unsafe_allow_html=True)
    
    elif selected == "üîç Nh·∫≠n d·∫°ng":
        st.markdown('<div class="section-header">üîç Nh·∫≠n d·∫°ng Vi√™n Thu·ªëc</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown("#### üì∏ T·∫£i l√™n h√¨nh ·∫£nh vi√™n thu·ªëc")
            uploaded_file = st.file_uploader(
                "Ch·ªçn h√¨nh ·∫£nh...",
                type=['png', 'jpg', 'jpeg'],
                help="H·ªó tr·ª£ ƒë·ªãnh d·∫°ng: PNG, JPG, JPEG"
            )
            
            if uploaded_file is not None:
                # Display uploaded image
                image = Image.open(uploaded_file)
                st.image(image, caption="H√¨nh ·∫£nh ƒë√£ t·∫£i l√™n", use_column_width=True)
                
                # Preprocess image
                image_tensor = preprocess_image(image)
                
                if image_tensor is not None:
                    st.success("‚úÖ H√¨nh ·∫£nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng!")
        
        with col2:
            st.markdown("#### üìù Nh·∫≠p text imprint")
            text_imprint = st.text_input(
                "Text tr√™n vi√™n thu·ªëc:",
                placeholder="V√≠ d·ª•: PILL123, MED500, RX10...",
                help="Nh·∫≠p text ƒë∆∞·ª£c in tr√™n vi√™n thu·ªëc (n·∫øu c√≥)"
            )
            
            st.markdown("#### ‚öôÔ∏è C√†i ƒë·∫∑t")
            show_features = st.checkbox("Hi·ªÉn th·ªã ph√¢n t√≠ch features", value=True)
            confidence_threshold = st.slider(
                "Ng∆∞·ª°ng ƒë·ªô tin c·∫≠y",
                min_value=0.0,
                max_value=1.0,
                value=0.5,
                step=0.05,
                help="Ch·ªâ hi·ªÉn th·ªã k·∫øt qu·∫£ c√≥ ƒë·ªô tin c·∫≠y tr√™n ng∆∞·ª°ng n√†y"
            )
        
        # Prediction button
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("üéØ Nh·∫≠n d·∫°ng vi√™n thu·ªëc", type="primary", use_container_width=True):
                if uploaded_file is not None and image_tensor is not None:
                    with st.spinner("üîÑ ƒêang ph√¢n t√≠ch..."):
                        # Add progress bar
                        progress_bar = st.progress(0)
                        for i in range(100):
                            time.sleep(0.01)
                            progress_bar.progress(i + 1)
                        
                        # Make prediction
                        results = predict_pill(
                            model, image_tensor, text_imprint or "",
                            device, tokenizer, sample_data
                        )
                        
                        if results:
                            # Filter by confidence threshold
                            filtered_predictions = [
                                pred for pred in results["predictions"]
                                if pred["confidence"] >= confidence_threshold
                            ]
                            
                            if filtered_predictions:
                                results["predictions"] = filtered_predictions
                                display_prediction_results(results)
                                
                                if show_features:
                                    display_feature_analysis(results["features"])
                            else:
                                st.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ª± ƒëo√°n n√†o ƒë·∫°t ng∆∞·ª°ng tin c·∫≠y {confidence_threshold:.1%}")
                        else:
                            st.error("‚ùå C√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh nh·∫≠n d·∫°ng")
                else:
                    st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i l√™n h√¨nh ·∫£nh tr∆∞·ªõc khi nh·∫≠n d·∫°ng")
    
    elif selected == "üìä Th·ªëng k√™":
        st.markdown('<div class="section-header">üìä Th·ªëng k√™ H·ªá th·ªëng</div>', unsafe_allow_html=True)
        
        # Sample statistics
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà Ph√¢n b·ªë d·ªØ li·ªáu m·∫´u")
            
            # Create sample distribution chart
            pill_types = [pill["name"] for pill in sample_data[:5]]
            confidences = [pill["confidence"] for pill in sample_data[:5]]
            
            fig = px.pie(
                values=confidences,
                names=pill_types,
                title="Ph√¢n b·ªë c√°c lo·∫°i thu·ªëc m·∫´u"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.markdown("#### üéØ Hi·ªáu su·∫•t Model")
            
            # Mock performance metrics
            metrics_data = {
                "Metric": ["Accuracy", "Precision", "Recall", "F1-Score"],
                "Training": [0.95, 0.94, 0.93, 0.94],
                "Validation": [0.89, 0.88, 0.87, 0.88]
            }
            
            metrics_df = pd.DataFrame(metrics_data)
            fig = px.bar(
                metrics_df,
                x="Metric",
                y=["Training", "Validation"],
                title="Hi·ªáu su·∫•t Model tr√™n t·∫≠p Train v√† Validation",
                barmode="group"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Training progress
        st.markdown("#### üìâ Qu√° tr√¨nh Training")
        
        # Mock training data
        epochs = list(range(1, 51))
        train_loss = [0.8 * np.exp(-x/10) + 0.1 + np.random.normal(0, 0.02) for x in epochs]
        val_loss = [0.9 * np.exp(-x/12) + 0.15 + np.random.normal(0, 0.03) for x in epochs]
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=epochs, y=train_loss, mode='lines', name='Training Loss'))
        fig.add_trace(go.Scatter(x=epochs, y=val_loss, mode='lines', name='Validation Loss'))
        fig.update_layout(
            title="Loss theo Epoch",
            xaxis_title="Epoch",
            yaxis_title="Loss"
        )
        st.plotly_chart(fig, use_container_width=True)
    
    elif selected == "‚ÑπÔ∏è Th√¥ng tin":
        st.markdown('<div class="section-header">‚ÑπÔ∏è Th√¥ng tin H·ªá th·ªëng</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("""
            ### üèóÔ∏è Ki·∫øn tr√∫c H·ªá th·ªëng
            
            #### ü§ñ Multimodal Transformer
            - **Visual Encoder**: Vision Transformer (ViT) ƒë·ªÉ x·ª≠ l√Ω h√¨nh ·∫£nh
            - **Text Encoder**: BERT ƒë·ªÉ x·ª≠ l√Ω text imprint
            - **Cross-modal Attention**: K·∫øt h·ª£p th√¥ng tin t·ª´ hai modality
            - **Fusion Layer**: T·ªïng h·ª£p features cu·ªëi c√πng
            - **Classifier**: Ph√¢n lo·∫°i vi√™n thu·ªëc
            
            #### üíæ Big Data Processing
            - **Apache Spark**: X·ª≠ l√Ω d·ªØ li·ªáu ph√¢n t√°n
            - **Rapids cuDF/cuML**: TƒÉng t·ªëc GPU
            - **Apache Parquet**: L∆∞u tr·ªØ d·ªØ li·ªáu hi·ªáu qu·∫£
            - **Elasticsearch**: Index v√† t√¨m ki·∫øm text
            
            #### üöÄ Tech Stack
            - **Framework**: PyTorch, Transformers, Streamlit
            - **Data**: PySpark, Pandas, NumPy
            - **Visualization**: Plotly, Matplotlib
            - **Deployment**: Docker, Kubernetes
            """)
        
        with col2:
            st.markdown("""
            ### üîß C·∫•u h√¨nh Model
            """)
            
            if config:
                with st.expander("üìã Model Configuration"):
                    st.json(config["model"])
                
                with st.expander("üéØ Training Configuration"):
                    st.json(config["training"])
                
                with st.expander("üíæ Data Configuration"):
                    st.json(config["data"])
        
        st.markdown("---")
        
        st.markdown("""
        ### üë• Nh√≥m ph√°t tri·ªÉn
        
        - **H·ªçc vi√™n**: [T√™n sinh vi√™n]
        - **M√¥n h·ªçc**: ƒê·ªì √°n ƒê·∫°i h·ªçc
        - **Tr∆∞·ªùng**: [T√™n tr∆∞·ªùng]
        - **NƒÉm**: 2025
        
        ### üìû Li√™n h·ªá
        
        - **Email**: [email@example.com]
        - **GitHub**: [github.com/username]
        - **Website**: [website.com]
        """)


if __name__ == "__main__":
    main()
